{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Posts Sentiment Analysis Using Word2Vec and lightbgm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Naphat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(path, include_all=False) -> tuple[list[list], list[int]]:\n",
    "    \"\"\"Return a list of lists for each comment and a list of labels if there is label\"\"\"\n",
    "    with open(path) as f:\n",
    "        header = f.readline().rstrip('\\n').split('\\t')\n",
    "        if 'Feedback' in header:\n",
    "            labels = []\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        texts = []\n",
    "        post_id = []\n",
    "        for line in f:\n",
    "            data = line.rstrip('\\n').split('\\t')\n",
    "            text = data[1]\n",
    "            processed_text = []\n",
    "            # Lowercasing\n",
    "            text = text.lower()\n",
    "            # Tokenization with NLTK\n",
    "            tokens = nltk.tokenize.word_tokenize(text)\n",
    "            # Remove words that are not alphabet, stop words and punctuations with NLTK\n",
    "            for token in tokens:\n",
    "                # if token.isalpha() and \\\n",
    "                if token not in nltk.corpus.stopwords.words('english') and \\\n",
    "                   token not in string.punctuation and \\\n",
    "                   len(token) > 1:\n",
    "                    processed_text.append(token)\n",
    "            # put processed text back into a list (remove the cases where nothing left after pre-processing)\n",
    "            if len(processed_text) > 0 or include_all:\n",
    "                texts.append(processed_text if processed_text else ['this'])\n",
    "                post_id.append(data[0])\n",
    "                if labels is not None:\n",
    "                    if data[2] == '1':\n",
    "                        labels.append(0)\n",
    "                    elif data[3] == '1':\n",
    "                        labels.append(1)\n",
    "                    elif data[4] == '1':\n",
    "                        labels.append(2)\n",
    "                    else:\n",
    "                        raise ValueError('No label specified!')\n",
    "    \n",
    "    return texts, labels, post_id\n",
    "\n",
    "\n",
    "def words_to_comment_vectors(proc_texts: list, word2vec_model, weighted_avg=False):\n",
    "    \"\"\"Transform processed comment to a vector representation\"\"\"\n",
    "    if weighted_avg:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            lowercase = True, \n",
    "            tokenizer = None,  # by default, it does word tokenization AND punctuation removal. You can replace it with a function that does other types of tokenziation\n",
    "            ngram_range = (1, 1),  # extract 1-gram (single tokens) and 2-gram (phrases of 2 words)\n",
    "            token_pattern = r\"[a-zA-Z0-9$&+,~_:;=?@#|/\\\\'`<>.^*()%!-]+\",\n",
    "            use_idf = True  # means that we want to get the TF-IDF, rather than just TF\n",
    "        )\n",
    "\n",
    "        corpus = [\" \".join(l) for l in proc_texts]\n",
    "\n",
    "        # Now apply it to the corpus and get the TF-IDF matrix\n",
    "        tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "        # Next, print it out in a nice readable format (this step is just to show you what it looks like, it's usually not needed)\n",
    "        df = pd.DataFrame(tfidf.todense(), columns = vectorizer.get_feature_names())\n",
    "        \n",
    "    vectors = []\n",
    "    for i, l in enumerate(proc_texts):\n",
    "        temp_vec = []\n",
    "        for word in l:\n",
    "            temp_vec.append(word2vec_model.wv[word])\n",
    "        \n",
    "        if weighted_avg:\n",
    "            weight = df.loc[i, proc_texts[i]].values\n",
    "            weight = weight / weight.sum()\n",
    "            vectors.append(np.sum(np.array(temp_vec) * weight.reshape(-1,1), axis=0))\n",
    "        else:\n",
    "            vectors.append(np.mean(temp_vec, axis=0))\n",
    "    \n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "def format_result(y_pred_prob, post_id):\n",
    "    \"\"\"Transform the result to the same format as required\"\"\"\n",
    "    y_pred_matrix = (y_pred_prob == y_pred_prob.max(axis=1)[:,None]).astype(int)\n",
    "    df_post_id = pd.DataFrame({'postId': post_id})\n",
    "    df_pred = pd.DataFrame(y_pred_matrix, columns=['Appreciation_pred', 'Complaint_pred', 'Feedback_pred'])\n",
    "\n",
    "    if df_post_id.shape[0] != df_pred.shape[0]:\n",
    "        raise ValueError('The shapes of y_pred_prob and post_id do not match')\n",
    "\n",
    "    return df_post_id.join(df_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get labeled and unlabeled data and perform some exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_texts, labels, _ = process_text('FB_posts_labeled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_text_unlabeled, _, post_id_test = process_text('FB_posts_unlabeled.txt', include_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7960"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(proc_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2039"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(proc_text_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['great'],\n",
       " ['yum', 'yum'],\n",
       " ['yummm'],\n",
       " ['sweet'],\n",
       " ['nice'],\n",
       " ['nice'],\n",
       " ['winner'],\n",
       " ['awesome'],\n",
       " ['yay'],\n",
       " ['gmo'],\n",
       " ['good'],\n",
       " ['thanks'],\n",
       " ['great'],\n",
       " ['thanks'],\n",
       " ['thanks'],\n",
       " ['like'],\n",
       " ['like'],\n",
       " ['lame'],\n",
       " ['echange'],\n",
       " ['like'],\n",
       " ['boo'],\n",
       " ['nice'],\n",
       " ['dislike'],\n",
       " ['thanks'],\n",
       " ['like'],\n",
       " ['like'],\n",
       " ['weak'],\n",
       " ['boo'],\n",
       " ['dislike'],\n",
       " ['liars'],\n",
       " ['ugh'],\n",
       " ['thanks'],\n",
       " ['yum'],\n",
       " ['yummy'],\n",
       " ['mmmmmm'],\n",
       " ['yum', 'yum'],\n",
       " ['love'],\n",
       " ['3v'],\n",
       " ['friendly'],\n",
       " ['like'],\n",
       " ['yea', 'southwest'],\n",
       " ['like'],\n",
       " ['congrats'],\n",
       " ['target'],\n",
       " ['like'],\n",
       " ['boycott'],\n",
       " ['nice'],\n",
       " [\"love.the.candles.i'm.a.candle.nut\", 'love.them'],\n",
       " ['great',\n",
       "  'beauty',\n",
       "  'deal',\n",
       "  'week',\n",
       "  'cvs',\n",
       "  'buy',\n",
       "  '20',\n",
       "  'pert',\n",
       "  'plus',\n",
       "  'let',\n",
       "  \"'s\",\n",
       "  'say',\n",
       "  'price',\n",
       "  '4.99',\n",
       "  '4.99+4.99=9.98',\n",
       "  '9.98-4.99=4.99',\n",
       "  '4.99-1.50-1.50=1.99',\n",
       "  '1.99-1.50=0.49',\n",
       "  'money',\n",
       "  'maker'],\n",
       " ['http', '//184.170.248.140/~kohls/', 'contact', 'real']]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_texts[:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Word2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model for embedding\n",
    "model = Word2Vec(sentences = proc_texts + proc_text_unlabeled,  # input should be a list of lists of tokens, like our output from preprocessing\n",
    "                 vector_size = 128,  # dimension of embedding (this parameter may be named size if you are using an older version of Gensim)\n",
    "                 window = 2,  # size of context window\n",
    "                 min_count = 1,  # remove very infrequent words\n",
    "                 sg = 1,  # skip-gram, set to 0 if you want CBOW\n",
    "                 workers = 4)  # parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05998771,  0.03020683, -0.02164491,  0.13270344, -0.02250056,\n",
       "       -0.24770862, -0.13834651,  0.07529167,  0.02148428, -0.26444113,\n",
       "        0.37745035, -0.20123658, -0.12367904,  0.18358901,  0.3133722 ,\n",
       "        0.07842196,  0.2808734 , -0.17672142,  0.3118569 ,  0.29917246,\n",
       "        0.3401665 ,  0.3653232 , -0.00482662,  0.10006938, -0.42488697,\n",
       "        0.1306782 , -0.25179565, -0.23342106, -0.05785725, -0.13708434,\n",
       "        0.19405954, -0.10312083, -0.2809174 ,  0.26817358,  0.12582755,\n",
       "        0.25577465,  0.16056074, -0.18008949,  0.588236  , -0.00090086,\n",
       "       -0.41429454,  0.3431835 , -0.1012907 , -0.17796099,  0.0069787 ,\n",
       "       -0.07736837,  0.0797224 , -0.14349657, -0.3857846 ,  0.20748998,\n",
       "       -0.08198371,  0.10756581, -0.17060216, -0.07641791,  0.08196544,\n",
       "        0.02937231,  0.3508993 , -0.02958911, -0.12427397, -0.2592816 ,\n",
       "        0.10441296, -0.10056689,  0.01867408, -0.40605795,  0.06571198,\n",
       "        0.46204072, -0.2601128 ,  0.09171007,  0.18541333,  0.31889528,\n",
       "       -0.03400268, -0.05837521, -0.3175219 , -0.07744218,  0.21815944,\n",
       "       -0.1262832 ,  0.27443534, -0.30329144, -0.00584142,  0.22271724,\n",
       "        0.00509497,  0.079753  ,  0.01787698,  0.3351603 ,  0.1462397 ,\n",
       "       -0.2160197 ,  0.22110799,  0.1861968 ,  0.09378508,  0.3953507 ,\n",
       "       -0.22791803, -0.10675741, -0.2832656 , -0.04730318,  0.5362661 ,\n",
       "       -0.11435933, -0.17903756, -0.18043642, -0.03041334,  0.36641476,\n",
       "       -0.28342524, -0.15756667, -0.41888377,  0.09568083,  0.06704501,\n",
       "        0.04342628, -0.04653729, -0.3232231 , -0.43829963, -0.40072456,\n",
       "       -0.3272486 , -0.10337822, -0.16094528, -0.16932166,  0.27974817,\n",
       "       -0.02364214,  0.33064532, -0.3184672 ,  0.29609025, -0.04707462,\n",
       "        0.19573288, -0.02240112, -0.12957105,  0.2578862 , -0.07500383,\n",
       "       -0.27712613,  0.18387842,  0.47678116], dtype=float32)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['nice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sad', 0.9835895895957947),\n",
       " ('thought', 0.9786968231201172),\n",
       " ('sorry', 0.9782928824424744),\n",
       " ('understand', 0.9767194986343384),\n",
       " ('upset', 0.9758037328720093),\n",
       " ('managers', 0.9751867055892944),\n",
       " ('idea', 0.9748938679695129),\n",
       " ('im', 0.9748210310935974),\n",
       " ('appreciate', 0.9726012945175171),\n",
       " ('knew', 0.9714584946632385)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = ['nice'], topn=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to transform each commend to a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Naphat\\anaconda3\\envs\\env_6420_conda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "comment_vects = words_to_comment_vectors(proc_texts, model, weighted_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20429446, -0.16239633,  0.16595899, ..., -0.44573224,\n",
       "         0.12689543,  0.40104273],\n",
       "       [-0.03522908,  0.06352112, -0.05770382, ..., -0.09318248,\n",
       "         0.14540175,  0.09063055],\n",
       "       [ 0.00221994, -0.00232594, -0.00434061, ...,  0.00286895,\n",
       "         0.0038337 , -0.00623277],\n",
       "       ...,\n",
       "       [-0.07692992,  0.10988255, -0.0948421 , ..., -0.25526255,\n",
       "         0.21775983,  0.2974752 ],\n",
       "       [-0.09168824,  0.10265933, -0.10190153, ..., -0.22839808,\n",
       "         0.23657685,  0.25559031],\n",
       "       [-0.06489915,  0.09159425, -0.08760808, ..., -0.22908652,\n",
       "         0.26196046,  0.25881368]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_vects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will use the vector representation of the comments to train a ligthgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32640\n",
      "[LightGBM] [Info] Number of data points in the train set: 7164, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score -1.359847\n",
      "[LightGBM] [Info] Start training from score -0.623596\n",
      "[LightGBM] [Info] Start training from score -1.573654\n"
     ]
    }
   ],
   "source": [
    "# Train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(comment_vects, np.array(labels), test_size=0.10, random_state=42)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "param = {'num_leaves': 20,\n",
    "         'objective': 'multiclass',\n",
    "         'max_depth': 35,\n",
    "         'metric': 'multi_logloss',\n",
    "         'num_class': 3}\n",
    "\n",
    "clf = lgb.train(param, train_data, num_boost_round=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a prediction using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = clf.predict(X_test)\n",
    "y_pred = np.argmax(clf.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[140  61  22]\n",
      " [ 41 342  31]\n",
      " [ 20  82  57]]\n",
      "accuracy = 0.6771356783919598\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('accuracy =', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is about 0.67 with the test set. I will use the current setting of hyperparameters for now and train the model again using the entire unlabelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32640\n",
      "[LightGBM] [Info] Number of data points in the train set: 7960, number of used features: 128\n",
      "[LightGBM] [Info] Start training from score -1.350753\n",
      "[LightGBM] [Info] Start training from score -0.626569\n",
      "[LightGBM] [Info] Start training from score -1.577297\n"
     ]
    }
   ],
   "source": [
    "# Train using all data\n",
    "train_data = lgb.Dataset(comment_vects, label=np.array(labels))\n",
    "\n",
    "param = {'num_leaves': 20,\n",
    "         'objective': 'multiclass',\n",
    "         'max_depth': 35,\n",
    "         'metric': 'multi_logloss',\n",
    "         'num_class': 3}\n",
    "\n",
    "clf = lgb.train(param, train_data, num_boost_round=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the model to predict predict labels for the unlabelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Naphat\\anaconda3\\envs\\env_6420_conda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "comment_vects_unl = words_to_comment_vectors(proc_text_unlabeled, model, weighted_avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = clf.predict(comment_vects_unl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_result(y_pred_prob, post_id_test).to_csv('output/040423-4.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitting the result here, I get about 0.67 average f-1 score, which is pretty low. for the next part I'll try using a transformer model (BERT) to perform the same task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_6420_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c04dfec66cb296653f474c7967ce5ba341f0cc4d8dd3daab895ffd0d6eddae38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
